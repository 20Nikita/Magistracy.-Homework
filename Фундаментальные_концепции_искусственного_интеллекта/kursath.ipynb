{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e641de91-8305-4708-aabe-dfd2851e8a32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Задачи:\n",
    "- Выбрать датасет и для него:\n",
    "- Включить ссылку на статью и выжимку из нее\n",
    "- Пример кода для загрузки тестового набора из кода\n",
    "- Пример данных с разметкой\n",
    "- Визуализировать экземпляр данных\n",
    "- Использовать библиотеку plotly\n",
    "- Использовать библиотеку tensorboard или аналог\n",
    "- Пример приминения готовой модели на этих данных\n",
    "- *Сделать препроцесинг другой моделью\n",
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c36c182c-d546-4993-b927-c55a54abe75a",
   "metadata": {},
   "source": [
    "# 300W_LP [Домашняя страница](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm?utm_source=www.tensorflow.org&utm_medium=referral)\n",
    "## Набор данных 300W-LP (Large Pose) основан на исходных изображениях 300W, <br /> расширенных алгоритмическими методами с 600 до 61 тыс. изображений. \n",
    "<img src=\"https://mlv4xkdrf2yq.i.optimole.com/cb:0l1_~a759/w:850/h:699/q:mauto/https://datagen.tech/app/uploads/2022/06/image1-2.png\">\n",
    "\n",
    "#### 300-W — это набор данных изображений лиц. Он содержит 300 изображений в помещении и 300 изображений на открытом воздухе с человеческими лицами, снятыми в дикой природе. Он охватывает разные лица, выражения лица, условия освещения, позы и размеры лица. Изображения были загружены с google.com с использованием поисковых запросов «вечеринка», «встреча», «протест» и «знаменитость». По сравнению с аналогичными наборами данных, 300W имеет более высокую долю частично закрытых изображений и необычные выражения лиц, такие как «удивление» и «крик». Изображения были аннотированы 68 точечными маркерами с использованием полуавтоматического метода. Размер изображений составляет от 48 КБ до 2 МБ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68638425",
   "metadata": {},
   "source": [
    "# Выжимка из [Статьи](https://arxiv.org/pdf/1511.07212.pdf)\n",
    "## Выравнивание лица в крупных позах: 3D-решение\n",
    "Большинство алгоритмов предназначены для лиц в малых и средних позах (ниже $45^∘$ ), не имея возможности выравнивать лица в крупных позах до $90^∘$. Предлагается структура выравнивания, называмая 3D Dense Face Alignment (3DDFA), в которой плотная 3D-модель лица подгоняется к изображению с помощью сверточной нейтральной сети (CNN)\n",
    "1) Предлагается подгонять к изображению 3D-модель плотного лица, а не модель формы разреженных ориентиров\n",
    "2) Чтобы разрешить процесс подбора в 3DDFA, мы предлагаем метод регрессии на основе каскадной сверточной нейтральной сети (CNN). В этой работе мы используем CNN, чтобы подогнать трехмерную модель лица к специально разработанной функции, а именно к коду проецированных нормализованных координат (PNCC). Кроме того, в качестве функции стоимости предлагается взвешенная стоимость параметра расстояния (WPDC).\n",
    "3) Чтобы обеспечить обучение 3DDFA, мы создаем [базу данных](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/) лиц, содержащую пары 2D-изображений лиц и 3D-моделей лиц."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28f41e7c",
   "metadata": {},
   "source": [
    "# Пример кода для загрузки набора данных"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8792c0",
   "metadata": {},
   "source": [
    "## Скачивание и распоковка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf98bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/0B7OEHD3T4eCkVGs0TkhUWFN6N1k/view?usp=sharing&resourcekey=0-WT5tO4TOCbNZY6r6z6WmOA\n",
    "import urllib.request \n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "pat = \"data/300W_LP.zip\"\n",
    "url = 'https://drive.google.com/u/0/uc?id=0B7OEHD3T4eCkVGs0TkhUWFN6N1k&export=download&confirm=t&uuid=c9c0ced8-6b2e-4a6d-851e-ef91955ab6ab&at=ACjLJWmJY8ijObanlNYugeanJKId:1671443143036' \n",
    "urllib.request.urlretrieve(url, pat)\n",
    "with zipfile.ZipFile(pat, 'r') as zip_file:\n",
    "    zip_file.extractall(\"data/\")\n",
    "shutil.rmtree(\"data/300W_LP/Code\")\n",
    "shutil.rmtree(\"data/300W_LP/landmarks\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f1010f9",
   "metadata": {},
   "source": [
    "## Создание нормального описания данных для задачи Head Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a896cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122450\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_ypr_from_mat(mat_path):\n",
    "    # Get yaw, pitch, roll from .mat annotation.\n",
    "    # They are in radians\n",
    "    mat = sio.loadmat(mat_path)\n",
    "    # [pitch yaw roll tdx tdy tdz scale_factor]\n",
    "    pre_pose_params = mat['Pose_Para'][0]\n",
    "    # Get [pitch, yaw, roll]\n",
    "    pose_params = pre_pose_params[:3]\n",
    "    return pose_params\n",
    "\n",
    "\n",
    "dirr = \"data/300W_LP\"\n",
    "papki = os.listdir(dirr)\n",
    "f = open(\"data/300W_LP/300W_LP_annotation.csv\", \"w\")\n",
    "f.write(\"snp,x,y,z\\n\")\n",
    "for i in range(len(papki)):\n",
    "    file = os.listdir(dirr + \"/\" + papki[i])\n",
    "    for j in range(len(file)):\n",
    "        t = file[j].split(\".\")\n",
    "        if t[1] == \"mat\":\n",
    "            pose = get_ypr_from_mat(dirr + \"/\" + papki[i] + \"/\" + file[j])\n",
    "            # And convert to degrees.\n",
    "            pitch = pose[0] * 180 / np.pi\n",
    "            yaw = pose[1] * 180 / np.pi\n",
    "            roll = pose[2] * 180 / np.pi\n",
    "            f.write(papki[i] + \"/\" + t[0] + '.jpg' + \",\" + str(yaw) + \",\" + str(pitch) + \",\" + str(roll)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee0373a0-4137-45ea-a686-e33896ccbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import plotly\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb8da1-3144-4a19-861e-dea61e87fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[300W-LP]: https://drive.google.com/file/d/0B7OEHD3T4eCkVGs0TkhUWFN6N1k/view?usp=sharing&resourcekey=0-WT5tO4TOCbNZY6r6z6WmOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee2d7b8-0362-44e7-987e-b1f8e84126c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b9ca36-fad0-470e-8793-556fcbbc431b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e94183-32ba-46d8-8eb6-f69280c5df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'pt2d', 'roi', 'Illum_Para', 'Color_Para', 'Tex_Para', 'Shape_Para', 'Exp_Para', 'Pose_Para'])\n",
      "[-0.08791836  0.00418041  0.0265818 ]\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'pt2d', 'roi', 'Illum_Para', 'Color_Para', 'Tex_Para', 'Shape_Para', 'Exp_Para', 'Pose_Para'])\n",
      "[-0.08791836  0.00418041  0.0265818 ]\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'pt2d', 'roi', 'Illum_Para', 'Color_Para', 'Tex_Para', 'Shape_Para', 'Exp_Para', 'Pose_Para'])\n",
      "[-0.08791836  0.00418041  0.0265818 ]\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'pt2d', 'roi', 'Illum_Para', 'Color_Para', 'Tex_Para', 'Shape_Para', 'Exp_Para', 'Pose_Para'])\n",
      "[-0.08791836  0.00418041  0.0265818 ]\n"
     ]
    }
   ],
   "source": [
    "print(get_ypr_from_mat(\"data/AFW/AFW_1051618982_1_0.mat\"))\n",
    "print(get_ypr_from_mat(\"data/AFW/AFW_1051618982_1_0.mat\"))\n",
    "print(get_ypr_from_mat(\"data/AFW/AFW_1051618982_1_0.mat\"))\n",
    "print(get_ypr_from_mat(\"data/AFW/AFW_1051618982_1_0.mat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3385d8-dbc6-4844-b2c2-1177ffcf20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_ypr_from_mat(mat_path):\n",
    "    # Get yaw, pitch, roll from .mat annotation.\n",
    "    # They are in radians\n",
    "    mat = sio.loadmat(mat_path)\n",
    "    # [pitch yaw roll tdx tdy tdz scale_factor]\n",
    "    pre_pose_params = mat['Pose_Para'][0]\n",
    "    # Get [pitch, yaw, roll]\n",
    "    pose_params = pre_pose_params[:3]\n",
    "    return pose_params\n",
    "\n",
    "\n",
    "dirr = \"300W_LP\"\n",
    "f = open(\"300W_LP_val_annotation.txt\", \"w\")\n",
    "f.write(\"snp,x,y,z\")\n",
    "papki = os.listdir(dirr)\n",
    "k=0\n",
    "for i in range(len(papki)):\n",
    "    file = os.listdir(dirr + \"/\" + papki[i])\n",
    "    for j in range(len(file)):\n",
    "        t = file[j].split(\".\")\n",
    "        try:\n",
    "            if t[1] == \"mat\":\n",
    "                k+=1\n",
    "                pose = get_ypr_from_mat(dirr + \"/\" + papki[i] + \"/\" + file[j])\n",
    "                # And convert to degrees.\n",
    "                pitch = pose[0] * 180 / np.pi\n",
    "                yaw = pose[1] * 180 / np.pi\n",
    "                roll = pose[2] * 180 / np.pi\n",
    "                f.write(papki[i] + \"/\" + t[0] + '.jpg' + \",\" + str(yaw) + \",\" + str(pitch) + \",\" + str(roll))\n",
    "        except:\n",
    "            print(t)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6dfc0d-ed40-4034-a1af-d1e0702696e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2022 The TensorFlow Datasets Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"300W-LP Dataset.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets.public_api as tfds\n",
    "\n",
    "_DATASET_URL = \"https://drive.google.com/uc?export=download&id=0B7OEHD3T4eCkVGs0TkhUWFN6N1k\"\n",
    "\n",
    "_PROJECT_URL = \"http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm\"\n",
    "\n",
    "\n",
    "class Builder(tfds.core.GeneratorBasedBuilder):\n",
    "  \"\"\"300W-LP dataset.\"\"\"\n",
    "\n",
    "  VERSION = tfds.core.Version(\"1.0.0\")\n",
    "\n",
    "  def _info(self):\n",
    "    return self.dataset_info_from_configs(\n",
    "        features=tfds.features.FeaturesDict({\n",
    "            \"image\":\n",
    "                tfds.features.Image(\n",
    "                    shape=(450, 450, 3), encoding_format=\"jpeg\"),\n",
    "            \"landmarks_origin\":\n",
    "                tfds.features.Tensor(shape=(68, 2), dtype=np.float32),\n",
    "            \"landmarks_2d\":\n",
    "                tfds.features.Tensor(shape=(68, 2), dtype=np.float32),\n",
    "            \"landmarks_3d\":\n",
    "                tfds.features.Tensor(shape=(68, 2), dtype=np.float32),\n",
    "            \"roi\":\n",
    "                tfds.features.Tensor(shape=(4,), dtype=np.float32),\n",
    "            \"illum_params\":\n",
    "                tfds.features.Tensor(shape=(10,), dtype=np.float32),\n",
    "            \"color_params\":\n",
    "                tfds.features.Tensor(shape=(7,), dtype=np.float32),\n",
    "            \"tex_params\":\n",
    "                tfds.features.Tensor(shape=(199,), dtype=np.float32),\n",
    "            \"shape_params\":\n",
    "                tfds.features.Tensor(shape=(199,), dtype=np.float32),\n",
    "            \"exp_params\":\n",
    "                tfds.features.Tensor(shape=(29,), dtype=np.float32),\n",
    "            \"pose_params\":\n",
    "                tfds.features.Tensor(shape=(7,), dtype=np.float32)\n",
    "        }),\n",
    "        homepage=_PROJECT_URL,\n",
    "    )\n",
    "\n",
    "  def _split_generators(self, dl_manager):\n",
    "    \"\"\"Returns SplitGenerators.\"\"\"\n",
    "    extracted_path = dl_manager.download_and_extract(_DATASET_URL)\n",
    "    return [\n",
    "        tfds.core.SplitGenerator(\n",
    "            name=tfds.Split.TRAIN,\n",
    "            gen_kwargs={\n",
    "                \"image_dir_path\": os.path.join(extracted_path, \"300W_LP\"),\n",
    "            }),\n",
    "    ]\n",
    "\n",
    "  def _generate_examples(self, image_dir_path):\n",
    "    \"\"\"Yields examples.\"\"\"\n",
    "    image_files = tf.io.gfile.glob(\n",
    "        pattern=os.path.join(image_dir_path, \"[!Code]*[!_Flip]/[!_]*.jpg\"))\n",
    "    label_files = [s.replace(\"jpg\", \"mat\") for s in image_files]\n",
    "    landmark_files = [\n",
    "        s.replace(\"300W_LP\", \"300W_LP/landmarks\").replace(\".jpg\", \"_pts.mat\")\n",
    "        for s in image_files\n",
    "    ]\n",
    "    for image_file, label_file, landmark_file in zip(image_files, label_files,\n",
    "                                                     landmark_files):\n",
    "      with tf.io.gfile.GFile(label_file, \"rb\") as f:\n",
    "        mat = tfds.core.lazy_imports.scipy.io.loadmat(f)\n",
    "      pt2d_origin = mat[\"pt2d\"].T\n",
    "      pt2d_origin = (pt2d_origin / 450.0).astype(np.float32)\n",
    "      roi = mat[\"roi\"].reshape(4).astype(np.float32)\n",
    "      illum_params = mat[\"Illum_Para\"].reshape([-1]).astype(np.float32)\n",
    "      color_params = mat[\"Color_Para\"].reshape([-1]).astype(np.float32)\n",
    "      tex_params = mat[\"Tex_Para\"].reshape([-1]).astype(np.float32)\n",
    "      shape_params = mat[\"Shape_Para\"].reshape([-1]).astype(np.float32)\n",
    "      exp_params = mat[\"Exp_Para\"].reshape([-1]).astype(np.float32)\n",
    "      pose_params = mat[\"Pose_Para\"].reshape([-1]).astype(np.float32)\n",
    "      with tf.io.gfile.GFile(landmark_file, \"rb\") as f:\n",
    "        ldm_mat = tfds.core.lazy_imports.scipy.io.loadmat(f)\n",
    "        pt2d = (ldm_mat[\"pts_2d\"] / 450.0).astype(np.float32)\n",
    "        pt3d = (ldm_mat[\"pts_3d\"] / 450.0).astype(np.float32)\n",
    "      record = {\n",
    "          \"image\": image_file,\n",
    "          \"landmarks_origin\": pt2d_origin,\n",
    "          \"landmarks_2d\": pt2d,\n",
    "          \"landmarks_3d\": pt3d,\n",
    "          \"roi\": roi,\n",
    "          \"illum_params\": illum_params,\n",
    "          \"color_params\": color_params,\n",
    "          \"tex_params\": tex_params,\n",
    "          \"shape_params\": shape_params,\n",
    "          \"exp_params\": exp_params,\n",
    "          \"pose_params\": pose_params\n",
    "      }\n",
    "      yield os.path.basename(image_file), record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be4e4dd-9174-4852-a40f-7f106ad61a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(image_dir_path):\n",
    "    \"\"\"Yields examples.\"\"\"\n",
    "    image_files = tf.io.gfile.glob(\n",
    "        pattern=os.path.join(image_dir_path, \"[!Code]*[!_Flip]/[!_]*.jpg\"))\n",
    "    label_files = [s.replace(\"jpg\", \"mat\") for s in image_files]\n",
    "    landmark_files = [\n",
    "        s.replace(\"300W_LP\", \"300W_LP/landmarks\").replace(\".jpg\", \"_pts.mat\")\n",
    "        for s in image_files\n",
    "    ]\n",
    "    print(image_files,label_files,landmark_files)\n",
    "    for image_file, label_file, landmark_file in zip(image_files, label_files,\n",
    "                                                     landmark_files):\n",
    "        with tf.io.gfile.GFile(label_file, \"rb\") as f:\n",
    "            mat = tfds.core.lazy_imports.scipy.io.loadmat(f)\n",
    "        pt2d_origin = mat[\"pt2d\"].T\n",
    "        pt2d_origin = (pt2d_origin / 450.0).astype(np.float32)\n",
    "        roi = mat[\"roi\"].reshape(4).astype(np.float32)\n",
    "        illum_params = mat[\"Illum_Para\"].reshape([-1]).astype(np.float32)\n",
    "        color_params = mat[\"Color_Para\"].reshape([-1]).astype(np.float32)\n",
    "        tex_params = mat[\"Tex_Para\"].reshape([-1]).astype(np.float32)\n",
    "        shape_params = mat[\"Shape_Para\"].reshape([-1]).astype(np.float32)\n",
    "        exp_params = mat[\"Exp_Para\"].reshape([-1]).astype(np.float32)\n",
    "        pose_params = mat[\"Pose_Para\"].reshape([-1]).astype(np.float32)\n",
    "        with tf.io.gfile.GFile(landmark_file, \"rb\") as f:\n",
    "            ldm_mat = tfds.core.lazy_imports.scipy.io.loadmat(f)\n",
    "            pt2d = (ldm_mat[\"pts_2d\"] / 450.0).astype(np.float32)\n",
    "            pt3d = (ldm_mat[\"pts_3d\"] / 450.0).astype(np.float32)\n",
    "        record = {\n",
    "          \"image\": image_file,\n",
    "          \"landmarks_origin\": pt2d_origin,\n",
    "          \"landmarks_2d\": pt2d,\n",
    "          \"landmarks_3d\": pt3d,\n",
    "          \"roi\": roi,\n",
    "          \"illum_params\": illum_params,\n",
    "          \"color_params\": color_params,\n",
    "          \"tex_params\": tex_params,\n",
    "          \"shape_params\": shape_params,\n",
    "          \"exp_params\": exp_params,\n",
    "          \"pose_params\": pose_params\n",
    "        }\n",
    "        return os.path.basename(image_file), record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1191ea7e-b24f-43bf-b014-ff2259c6317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [] []\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(generate_examples(\"data/AFW/AFW_1051618982_1_0.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29815a-8f49-42f3-8372-e265f1f7231b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "67f1dc6f6f712f7142079021955b91e049abb319dcfdc9eed010dd73dd4d845d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
